{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN027PxxGx36sxEe2rI/SP3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwarang97/Image_classification/blob/main/lenet_5_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y4ku3Bb97bk1"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.activation import Softmax\n",
        "import torch\n",
        "from torch import nn \n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor() # 이미지, 배열을 텐서로 변경하고, 픽셀값을 정규하시켜줌\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor() \n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=1)\n",
        "test_dataloader = DataLoader(test_data, batch_size=1)\n",
        "\n",
        "class Lenet_5(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Lenet_5, self).__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.conv_sigmoid_stack = nn.Sequential(\n",
        "        nn.Conv2d(1, 6, 5, padding=2), # input 1*32*32, output 6*28*28, same padding\n",
        "        nn.Sigmoid(),\n",
        "        nn.AvgPool2d(2),\n",
        "        nn.Conv2d(6, 16, 5),\n",
        "        nn.Sigmoid(),\n",
        "        nn.AvgPool2d(2),\n",
        "    )\n",
        "\n",
        "    self.linear_sigmoid_stack = nn.Sequential(\n",
        "      nn.Linear(16*5*5, 84),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(84, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.conv_sigmoid_stack(x)\n",
        "    x2 = self.flatten(x1)\n",
        "    logits = self.linear_sigmoid_stack(x2)\n",
        "    return logits\n",
        "\n",
        "  # def __init__(self):\n",
        "  #       super(Lenet_5, self).__init__()\n",
        "  #       # 입력 이미지 채널 1개, 출력 채널 6개, 5x5의 정사각 컨볼루션 행렬\n",
        "  #       # 컨볼루션 커널 정의\n",
        "  #       self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "  #       self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "  #       # 아핀(affine) 연산: y = Wx + b\n",
        "  #       self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5은 이미지 차원에 해당\n",
        "  #       self.fc2 = nn.Linear(120, 84)\n",
        "  #       self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  # def forward(self, x):\n",
        "  #     # (2, 2) 크기 윈도우에 대해 맥스 풀링(max pooling)\n",
        "  #     x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "  #     # 크기가 제곱수라면, 하나의 숫자만을 특정(specify)\n",
        "  #     x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "  #     x = torch.flatten(x, 1) # 배치 차원을 제외한 모든 차원을 하나로 평탄화(flatten)\n",
        "  #     x = F.relu(self.fc1(x))\n",
        "  #     x = F.relu(self.fc2(x))\n",
        "  #     x = self.fc3(x)\n",
        "  #     return x"
      ],
      "metadata": {
        "id": "4SBroAni8BUI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader, 1):\n",
        "\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 1000 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n')"
      ],
      "metadata": {
        "id": "lIQ8bTqxDOBJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Lenet_5().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLDxctZ8Ivyg",
        "outputId": "d6b5fa22-1042-4ffe-805f-e554146c3e9c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lenet_5(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (conv_sigmoid_stack): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): Sigmoid()\n",
            "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): Sigmoid()\n",
            "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  )\n",
            "  (linear_sigmoid_stack): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=84, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 1\n",
        "epochs = 10\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for t in range(epochs):\n",
        "  print(f'Epoch {t+1}\\n-----------------------------------')\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "print('Done!!!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETUBuGVh8f12",
        "outputId": "4baaa9f1-0f1b-4e7a-d7d6-7a7a9b87c609"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-----------------------------------\n",
            "loss: 2.307811 [ 1000/60000]\n",
            "loss: 2.344677 [ 2000/60000]\n",
            "loss: 2.376512 [ 3000/60000]\n",
            "loss: 2.238244 [ 4000/60000]\n",
            "loss: 2.430893 [ 5000/60000]\n",
            "loss: 2.229689 [ 6000/60000]\n",
            "loss: 2.387041 [ 7000/60000]\n",
            "loss: 2.503430 [ 8000/60000]\n",
            "loss: 2.490960 [ 9000/60000]\n",
            "loss: 2.237942 [10000/60000]\n",
            "loss: 2.377086 [11000/60000]\n",
            "loss: 2.415211 [12000/60000]\n",
            "loss: 2.314795 [13000/60000]\n",
            "loss: 2.353487 [14000/60000]\n",
            "loss: 2.248267 [15000/60000]\n",
            "loss: 2.130532 [16000/60000]\n",
            "loss: 2.297227 [17000/60000]\n",
            "loss: 2.313713 [18000/60000]\n",
            "loss: 2.294303 [19000/60000]\n",
            "loss: 2.421269 [20000/60000]\n",
            "loss: 2.368023 [21000/60000]\n",
            "loss: 2.212766 [22000/60000]\n",
            "loss: 2.338611 [23000/60000]\n",
            "loss: 2.372835 [24000/60000]\n",
            "loss: 2.278524 [25000/60000]\n",
            "loss: 2.383859 [26000/60000]\n",
            "loss: 2.431303 [27000/60000]\n",
            "loss: 2.371299 [28000/60000]\n",
            "loss: 2.294688 [29000/60000]\n",
            "loss: 2.245003 [30000/60000]\n",
            "loss: 2.361755 [31000/60000]\n",
            "loss: 2.390866 [32000/60000]\n",
            "loss: 2.296299 [33000/60000]\n",
            "loss: 2.300160 [34000/60000]\n",
            "loss: 2.414707 [35000/60000]\n",
            "loss: 2.266481 [36000/60000]\n",
            "loss: 2.371822 [37000/60000]\n",
            "loss: 2.321078 [38000/60000]\n",
            "loss: 2.433570 [39000/60000]\n",
            "loss: 2.343781 [40000/60000]\n",
            "loss: 2.082870 [41000/60000]\n",
            "loss: 2.328245 [42000/60000]\n",
            "loss: 2.151383 [43000/60000]\n",
            "loss: 2.294912 [44000/60000]\n",
            "loss: 2.283539 [45000/60000]\n",
            "loss: 2.230078 [46000/60000]\n",
            "loss: 2.434706 [47000/60000]\n",
            "loss: 2.204270 [48000/60000]\n",
            "loss: 2.299843 [49000/60000]\n",
            "loss: 2.445477 [50000/60000]\n",
            "loss: 2.205660 [51000/60000]\n",
            "loss: 2.319917 [52000/60000]\n",
            "loss: 2.531980 [53000/60000]\n",
            "loss: 2.285236 [54000/60000]\n",
            "loss: 2.272271 [55000/60000]\n",
            "loss: 2.200078 [56000/60000]\n",
            "loss: 2.338906 [57000/60000]\n",
            "loss: 2.180105 [58000/60000]\n",
            "loss: 2.280615 [59000/60000]\n",
            "loss: 2.365980 [60000/60000]\n",
            "Test Error: \n",
            " Accuracy: 9.6%, Avg loss: 2.301455 \n",
            "\n",
            "Epoch 2\n",
            "-----------------------------------\n",
            "loss: 2.333595 [ 1000/60000]\n",
            "loss: 2.349696 [ 2000/60000]\n",
            "loss: 2.380448 [ 3000/60000]\n",
            "loss: 2.253906 [ 4000/60000]\n",
            "loss: 2.408526 [ 5000/60000]\n",
            "loss: 2.240918 [ 6000/60000]\n",
            "loss: 2.366069 [ 7000/60000]\n",
            "loss: 2.486348 [ 8000/60000]\n",
            "loss: 2.445259 [ 9000/60000]\n",
            "loss: 2.235182 [10000/60000]\n",
            "loss: 2.355046 [11000/60000]\n",
            "loss: 2.388451 [12000/60000]\n",
            "loss: 2.302398 [13000/60000]\n",
            "loss: 2.359018 [14000/60000]\n",
            "loss: 2.244390 [15000/60000]\n",
            "loss: 2.133188 [16000/60000]\n",
            "loss: 2.295084 [17000/60000]\n",
            "loss: 2.303292 [18000/60000]\n",
            "loss: 2.281058 [19000/60000]\n",
            "loss: 2.406008 [20000/60000]\n",
            "loss: 2.351205 [21000/60000]\n",
            "loss: 2.245889 [22000/60000]\n",
            "loss: 2.314599 [23000/60000]\n",
            "loss: 2.346615 [24000/60000]\n",
            "loss: 2.299460 [25000/60000]\n",
            "loss: 2.374860 [26000/60000]\n",
            "loss: 2.416543 [27000/60000]\n",
            "loss: 2.369145 [28000/60000]\n",
            "loss: 2.308693 [29000/60000]\n",
            "loss: 2.231977 [30000/60000]\n",
            "loss: 2.351223 [31000/60000]\n",
            "loss: 2.402558 [32000/60000]\n",
            "loss: 2.313055 [33000/60000]\n",
            "loss: 2.298889 [34000/60000]\n",
            "loss: 2.390034 [35000/60000]\n",
            "loss: 2.286126 [36000/60000]\n",
            "loss: 2.381439 [37000/60000]\n",
            "loss: 2.324636 [38000/60000]\n",
            "loss: 2.422966 [39000/60000]\n",
            "loss: 2.361605 [40000/60000]\n",
            "loss: 2.102415 [41000/60000]\n",
            "loss: 2.295182 [42000/60000]\n",
            "loss: 2.141709 [43000/60000]\n",
            "loss: 2.298147 [44000/60000]\n",
            "loss: 2.318639 [45000/60000]\n",
            "loss: 2.237305 [46000/60000]\n",
            "loss: 2.440762 [47000/60000]\n",
            "loss: 2.213173 [48000/60000]\n",
            "loss: 2.296729 [49000/60000]\n",
            "loss: 2.419257 [50000/60000]\n",
            "loss: 2.227913 [51000/60000]\n",
            "loss: 2.310614 [52000/60000]\n",
            "loss: 2.498272 [53000/60000]\n",
            "loss: 2.288897 [54000/60000]\n",
            "loss: 2.281664 [55000/60000]\n",
            "loss: 2.207256 [56000/60000]\n",
            "loss: 2.337650 [57000/60000]\n",
            "loss: 2.216187 [58000/60000]\n",
            "loss: 2.292703 [59000/60000]\n",
            "loss: 2.360106 [60000/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.3%, Avg loss: 2.300328 \n",
            "\n",
            "Epoch 3\n",
            "-----------------------------------\n",
            "loss: 2.332302 [ 1000/60000]\n",
            "loss: 2.348549 [ 2000/60000]\n",
            "loss: 2.383718 [ 3000/60000]\n",
            "loss: 2.261120 [ 4000/60000]\n",
            "loss: 2.391315 [ 5000/60000]\n",
            "loss: 2.252603 [ 6000/60000]\n",
            "loss: 2.348348 [ 7000/60000]\n",
            "loss: 2.468154 [ 8000/60000]\n",
            "loss: 2.413421 [ 9000/60000]\n",
            "loss: 2.235458 [10000/60000]\n",
            "loss: 2.335044 [11000/60000]\n",
            "loss: 2.364824 [12000/60000]\n",
            "loss: 2.293153 [13000/60000]\n",
            "loss: 2.361512 [14000/60000]\n",
            "loss: 2.243993 [15000/60000]\n",
            "loss: 2.135198 [16000/60000]\n",
            "loss: 2.291806 [17000/60000]\n",
            "loss: 2.297152 [18000/60000]\n",
            "loss: 2.277543 [19000/60000]\n",
            "loss: 2.394638 [20000/60000]\n",
            "loss: 2.339090 [21000/60000]\n",
            "loss: 2.266654 [22000/60000]\n",
            "loss: 2.295626 [23000/60000]\n",
            "loss: 2.328656 [24000/60000]\n",
            "loss: 2.311376 [25000/60000]\n",
            "loss: 2.364527 [26000/60000]\n",
            "loss: 2.404471 [27000/60000]\n",
            "loss: 2.362899 [28000/60000]\n",
            "loss: 2.318179 [29000/60000]\n",
            "loss: 2.219036 [30000/60000]\n",
            "loss: 2.344876 [31000/60000]\n",
            "loss: 2.410485 [32000/60000]\n",
            "loss: 2.319936 [33000/60000]\n",
            "loss: 2.295292 [34000/60000]\n",
            "loss: 2.375200 [35000/60000]\n",
            "loss: 2.296841 [36000/60000]\n",
            "loss: 2.391980 [37000/60000]\n",
            "loss: 2.322739 [38000/60000]\n",
            "loss: 2.409131 [39000/60000]\n",
            "loss: 2.371332 [40000/60000]\n",
            "loss: 2.109117 [41000/60000]\n",
            "loss: 2.273974 [42000/60000]\n",
            "loss: 2.132529 [43000/60000]\n",
            "loss: 2.297286 [44000/60000]\n",
            "loss: 2.339273 [45000/60000]\n",
            "loss: 2.235009 [46000/60000]\n",
            "loss: 2.440080 [47000/60000]\n",
            "loss: 2.218553 [48000/60000]\n",
            "loss: 2.297874 [49000/60000]\n",
            "loss: 2.399573 [50000/60000]\n",
            "loss: 2.238288 [51000/60000]\n",
            "loss: 2.298396 [52000/60000]\n",
            "loss: 2.475721 [53000/60000]\n",
            "loss: 2.291910 [54000/60000]\n",
            "loss: 2.277426 [55000/60000]\n",
            "loss: 2.209503 [56000/60000]\n",
            "loss: 2.338689 [57000/60000]\n",
            "loss: 2.233387 [58000/60000]\n",
            "loss: 2.297122 [59000/60000]\n",
            "loss: 2.355098 [60000/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.3%, Avg loss: 2.296936 \n",
            "\n",
            "Epoch 4\n",
            "-----------------------------------\n",
            "loss: 2.331452 [ 1000/60000]\n",
            "loss: 2.334100 [ 2000/60000]\n",
            "loss: 2.385548 [ 3000/60000]\n",
            "loss: 2.261122 [ 4000/60000]\n",
            "loss: 2.380503 [ 5000/60000]\n",
            "loss: 2.258508 [ 6000/60000]\n",
            "loss: 2.336475 [ 7000/60000]\n",
            "loss: 2.453331 [ 8000/60000]\n",
            "loss: 2.394307 [ 9000/60000]\n",
            "loss: 2.233747 [10000/60000]\n",
            "loss: 2.318523 [11000/60000]\n",
            "loss: 2.350511 [12000/60000]\n",
            "loss: 2.287764 [13000/60000]\n",
            "loss: 2.359722 [14000/60000]\n",
            "loss: 2.240525 [15000/60000]\n",
            "loss: 2.127074 [16000/60000]\n",
            "loss: 2.285357 [17000/60000]\n",
            "loss: 2.293916 [18000/60000]\n",
            "loss: 2.276416 [19000/60000]\n",
            "loss: 2.383631 [20000/60000]\n",
            "loss: 2.331668 [21000/60000]\n",
            "loss: 2.269971 [22000/60000]\n",
            "loss: 2.285746 [23000/60000]\n",
            "loss: 2.316551 [24000/60000]\n",
            "loss: 2.309356 [25000/60000]\n",
            "loss: 2.351269 [26000/60000]\n",
            "loss: 2.377669 [27000/60000]\n",
            "loss: 2.352188 [28000/60000]\n",
            "loss: 2.315268 [29000/60000]\n",
            "loss: 2.197956 [30000/60000]\n",
            "loss: 2.340810 [31000/60000]\n",
            "loss: 2.413493 [32000/60000]\n",
            "loss: 2.310003 [33000/60000]\n",
            "loss: 2.289934 [34000/60000]\n",
            "loss: 2.359043 [35000/60000]\n",
            "loss: 2.272378 [36000/60000]\n",
            "loss: 2.392974 [37000/60000]\n",
            "loss: 2.308211 [38000/60000]\n",
            "loss: 2.400429 [39000/60000]\n",
            "loss: 2.371782 [40000/60000]\n",
            "loss: 2.053210 [41000/60000]\n",
            "loss: 2.237543 [42000/60000]\n",
            "loss: 2.076767 [43000/60000]\n",
            "loss: 2.286260 [44000/60000]\n",
            "loss: 2.329674 [45000/60000]\n",
            "loss: 2.177893 [46000/60000]\n",
            "loss: 2.439923 [47000/60000]\n",
            "loss: 2.180701 [48000/60000]\n",
            "loss: 2.285299 [49000/60000]\n",
            "loss: 2.403526 [50000/60000]\n",
            "loss: 2.201053 [51000/60000]\n",
            "loss: 2.199929 [52000/60000]\n",
            "loss: 2.491904 [53000/60000]\n",
            "loss: 2.293959 [54000/60000]\n",
            "loss: 2.101057 [55000/60000]\n",
            "loss: 2.187552 [56000/60000]\n",
            "loss: 2.341792 [57000/60000]\n",
            "loss: 2.139352 [58000/60000]\n",
            "loss: 2.269233 [59000/60000]\n",
            "loss: 2.364047 [60000/60000]\n",
            "Test Error: \n",
            " Accuracy: 38.9%, Avg loss: 2.232940 \n",
            "\n",
            "Epoch 5\n",
            "-----------------------------------\n",
            "loss: 2.311900 [ 1000/60000]\n",
            "loss: 1.959043 [ 2000/60000]\n",
            "loss: 2.374819 [ 3000/60000]\n",
            "loss: 2.179276 [ 4000/60000]\n",
            "loss: 2.386923 [ 5000/60000]\n",
            "loss: 2.195241 [ 6000/60000]\n",
            "loss: 2.293047 [ 7000/60000]\n",
            "loss: 2.399978 [ 8000/60000]\n",
            "loss: 2.342586 [ 9000/60000]\n",
            "loss: 2.109363 [10000/60000]\n",
            "loss: 2.105688 [11000/60000]\n",
            "loss: 2.304798 [12000/60000]\n",
            "loss: 2.291556 [13000/60000]\n",
            "loss: 2.237868 [14000/60000]\n",
            "loss: 2.026746 [15000/60000]\n",
            "loss: 1.319692 [16000/60000]\n",
            "loss: 2.036861 [17000/60000]\n",
            "loss: 2.184353 [18000/60000]\n",
            "loss: 2.069371 [19000/60000]\n",
            "loss: 2.055712 [20000/60000]\n",
            "loss: 2.131430 [21000/60000]\n",
            "loss: 1.793446 [22000/60000]\n",
            "loss: 2.052227 [23000/60000]\n",
            "loss: 1.836046 [24000/60000]\n",
            "loss: 1.900355 [25000/60000]\n",
            "loss: 1.622796 [26000/60000]\n",
            "loss: 0.823058 [27000/60000]\n",
            "loss: 1.917994 [28000/60000]\n",
            "loss: 1.997354 [29000/60000]\n",
            "loss: 1.071144 [30000/60000]\n",
            "loss: 2.004712 [31000/60000]\n",
            "loss: 2.085817 [32000/60000]\n",
            "loss: 1.319773 [33000/60000]\n",
            "loss: 2.019369 [34000/60000]\n",
            "loss: 1.616515 [35000/60000]\n",
            "loss: 0.707756 [36000/60000]\n",
            "loss: 1.970946 [37000/60000]\n",
            "loss: 0.709380 [38000/60000]\n",
            "loss: 1.784259 [39000/60000]\n",
            "loss: 2.332508 [40000/60000]\n",
            "loss: 0.248597 [41000/60000]\n",
            "loss: 0.696622 [42000/60000]\n",
            "loss: 0.197905 [43000/60000]\n",
            "loss: 1.245342 [44000/60000]\n",
            "loss: 1.272847 [45000/60000]\n",
            "loss: 0.336728 [46000/60000]\n",
            "loss: 2.064318 [47000/60000]\n",
            "loss: 0.991773 [48000/60000]\n",
            "loss: 0.696841 [49000/60000]\n",
            "loss: 1.684850 [50000/60000]\n",
            "loss: 0.281236 [51000/60000]\n",
            "loss: 0.100123 [52000/60000]\n",
            "loss: 1.869832 [53000/60000]\n",
            "loss: 1.893120 [54000/60000]\n",
            "loss: 0.080447 [55000/60000]\n",
            "loss: 0.798992 [56000/60000]\n",
            "loss: 2.253867 [57000/60000]\n",
            "loss: 0.090401 [58000/60000]\n",
            "loss: 0.834729 [59000/60000]\n",
            "loss: 1.319478 [60000/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.8%, Avg loss: 0.859392 \n",
            "\n",
            "Epoch 6\n",
            "-----------------------------------\n",
            "loss: 0.718242 [ 1000/60000]\n",
            "loss: 0.224737 [ 2000/60000]\n",
            "loss: 1.033608 [ 3000/60000]\n",
            "loss: 0.117301 [ 4000/60000]\n",
            "loss: 0.517447 [ 5000/60000]\n",
            "loss: 0.618832 [ 6000/60000]\n",
            "loss: 0.628950 [ 7000/60000]\n",
            "loss: 0.247250 [ 8000/60000]\n",
            "loss: 0.740916 [ 9000/60000]\n",
            "loss: 0.317048 [10000/60000]\n",
            "loss: 0.109749 [11000/60000]\n",
            "loss: 0.549736 [12000/60000]\n",
            "loss: 1.318358 [13000/60000]\n",
            "loss: 0.341989 [14000/60000]\n",
            "loss: 0.584117 [15000/60000]\n",
            "loss: 0.171065 [16000/60000]\n",
            "loss: 0.408321 [17000/60000]\n",
            "loss: 0.546712 [18000/60000]\n",
            "loss: 0.811316 [19000/60000]\n",
            "loss: 0.126173 [20000/60000]\n",
            "loss: 0.696927 [21000/60000]\n",
            "loss: 0.525298 [22000/60000]\n",
            "loss: 0.774041 [23000/60000]\n",
            "loss: 0.403510 [24000/60000]\n",
            "loss: 0.801293 [25000/60000]\n",
            "loss: 0.090147 [26000/60000]\n",
            "loss: 0.087316 [27000/60000]\n",
            "loss: 0.207442 [28000/60000]\n",
            "loss: 0.599841 [29000/60000]\n",
            "loss: 0.371476 [30000/60000]\n",
            "loss: 0.620789 [31000/60000]\n",
            "loss: 0.917442 [32000/60000]\n",
            "loss: 0.169016 [33000/60000]\n",
            "loss: 1.317422 [34000/60000]\n",
            "loss: 0.435576 [35000/60000]\n",
            "loss: 0.137820 [36000/60000]\n",
            "loss: 1.092137 [37000/60000]\n",
            "loss: 0.109134 [38000/60000]\n",
            "loss: 0.648760 [39000/60000]\n",
            "loss: 2.651413 [40000/60000]\n",
            "loss: 0.071548 [41000/60000]\n",
            "loss: 0.228306 [42000/60000]\n",
            "loss: 0.034495 [43000/60000]\n",
            "loss: 0.852204 [44000/60000]\n",
            "loss: 0.219440 [45000/60000]\n",
            "loss: 0.109013 [46000/60000]\n",
            "loss: 2.379265 [47000/60000]\n",
            "loss: 0.365654 [48000/60000]\n",
            "loss: 0.151826 [49000/60000]\n",
            "loss: 0.565994 [50000/60000]\n",
            "loss: 0.059840 [51000/60000]\n",
            "loss: 0.043568 [52000/60000]\n",
            "loss: 0.883521 [53000/60000]\n",
            "loss: 3.957147 [54000/60000]\n",
            "loss: 0.027893 [55000/60000]\n",
            "loss: 0.123889 [56000/60000]\n",
            "loss: 2.195861 [57000/60000]\n",
            "loss: 0.005226 [58000/60000]\n",
            "loss: 0.490699 [59000/60000]\n",
            "loss: 0.523467 [60000/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.1%, Avg loss: 0.508603 \n",
            "\n",
            "Epoch 7\n",
            "-----------------------------------\n",
            "loss: 0.213899 [ 1000/60000]\n",
            "loss: 0.080215 [ 2000/60000]\n",
            "loss: 0.220706 [ 3000/60000]\n",
            "loss: 0.024556 [ 4000/60000]\n",
            "loss: 0.107265 [ 5000/60000]\n",
            "loss: 0.225265 [ 6000/60000]\n",
            "loss: 0.273892 [ 7000/60000]\n",
            "loss: 0.062671 [ 8000/60000]\n",
            "loss: 0.308620 [ 9000/60000]\n",
            "loss: 0.092060 [10000/60000]\n",
            "loss: 0.011401 [11000/60000]\n",
            "loss: 0.560657 [12000/60000]\n",
            "loss: 0.835968 [13000/60000]\n",
            "loss: 0.157064 [14000/60000]\n",
            "loss: 0.568446 [15000/60000]\n",
            "loss: 0.097649 [16000/60000]\n",
            "loss: 0.126410 [17000/60000]\n",
            "loss: 0.174183 [18000/60000]\n",
            "loss: 0.592718 [19000/60000]\n",
            "loss: 0.048813 [20000/60000]\n",
            "loss: 0.644310 [21000/60000]\n",
            "loss: 0.314122 [22000/60000]\n",
            "loss: 0.230492 [23000/60000]\n",
            "loss: 0.264798 [24000/60000]\n",
            "loss: 0.591361 [25000/60000]\n",
            "loss: 0.033119 [26000/60000]\n",
            "loss: 0.054896 [27000/60000]\n",
            "loss: 0.033776 [28000/60000]\n",
            "loss: 0.189581 [29000/60000]\n",
            "loss: 0.241505 [30000/60000]\n",
            "loss: 0.295635 [31000/60000]\n",
            "loss: 0.424293 [32000/60000]\n",
            "loss: 0.082214 [33000/60000]\n",
            "loss: 0.927135 [34000/60000]\n",
            "loss: 0.099666 [35000/60000]\n",
            "loss: 0.187948 [36000/60000]\n",
            "loss: 0.687369 [37000/60000]\n",
            "loss: 0.055873 [38000/60000]\n",
            "loss: 0.406780 [39000/60000]\n",
            "loss: 2.730206 [40000/60000]\n",
            "loss: 0.039843 [41000/60000]\n",
            "loss: 0.122838 [42000/60000]\n",
            "loss: 0.021712 [43000/60000]\n",
            "loss: 0.827987 [44000/60000]\n",
            "loss: 0.070278 [45000/60000]\n",
            "loss: 0.066267 [46000/60000]\n",
            "loss: 1.846410 [47000/60000]\n",
            "loss: 0.243066 [48000/60000]\n",
            "loss: 0.107593 [49000/60000]\n",
            "loss: 0.254411 [50000/60000]\n",
            "loss: 0.038990 [51000/60000]\n",
            "loss: 0.044081 [52000/60000]\n",
            "loss: 0.308118 [53000/60000]\n",
            "loss: 5.092267 [54000/60000]\n",
            "loss: 0.017938 [55000/60000]\n",
            "loss: 0.040881 [56000/60000]\n",
            "loss: 2.152184 [57000/60000]\n",
            "loss: 0.002552 [58000/60000]\n",
            "loss: 0.377420 [59000/60000]\n",
            "loss: 0.241525 [60000/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.387828 \n",
            "\n",
            "Epoch 8\n",
            "-----------------------------------\n",
            "loss: 0.116200 [ 1000/60000]\n",
            "loss: 0.029726 [ 2000/60000]\n",
            "loss: 0.052324 [ 3000/60000]\n",
            "loss: 0.015844 [ 4000/60000]\n",
            "loss: 0.057471 [ 5000/60000]\n",
            "loss: 0.141091 [ 6000/60000]\n",
            "loss: 0.174839 [ 7000/60000]\n",
            "loss: 0.025039 [ 8000/60000]\n",
            "loss: 0.186184 [ 9000/60000]\n",
            "loss: 0.057598 [10000/60000]\n",
            "loss: 0.004480 [11000/60000]\n",
            "loss: 0.840244 [12000/60000]\n",
            "loss: 0.618371 [13000/60000]\n",
            "loss: 0.121310 [14000/60000]\n",
            "loss: 0.681804 [15000/60000]\n",
            "loss: 0.075211 [16000/60000]\n",
            "loss: 0.077303 [17000/60000]\n",
            "loss: 0.100473 [18000/60000]\n",
            "loss: 0.428443 [19000/60000]\n",
            "loss: 0.035174 [20000/60000]\n",
            "loss: 0.631139 [21000/60000]\n",
            "loss: 0.163013 [22000/60000]\n",
            "loss: 0.085012 [23000/60000]\n",
            "loss: 0.125990 [24000/60000]\n",
            "loss: 0.490639 [25000/60000]\n",
            "loss: 0.023138 [26000/60000]\n",
            "loss: 0.036330 [27000/60000]\n",
            "loss: 0.013445 [28000/60000]\n",
            "loss: 0.074114 [29000/60000]\n",
            "loss: 0.178942 [30000/60000]\n",
            "loss: 0.189439 [31000/60000]\n",
            "loss: 0.191994 [32000/60000]\n",
            "loss: 0.052597 [33000/60000]\n",
            "loss: 0.693085 [34000/60000]\n",
            "loss: 0.039995 [35000/60000]\n",
            "loss: 0.177153 [36000/60000]\n",
            "loss: 0.513595 [37000/60000]\n",
            "loss: 0.039603 [38000/60000]\n",
            "loss: 0.261935 [39000/60000]\n",
            "loss: 2.722577 [40000/60000]\n",
            "loss: 0.027357 [41000/60000]\n",
            "loss: 0.094323 [42000/60000]\n",
            "loss: 0.017993 [43000/60000]\n",
            "loss: 0.771990 [44000/60000]\n",
            "loss: 0.038970 [45000/60000]\n",
            "loss: 0.049406 [46000/60000]\n",
            "loss: 1.216468 [47000/60000]\n",
            "loss: 0.192379 [48000/60000]\n",
            "loss: 0.098933 [49000/60000]\n",
            "loss: 0.163771 [50000/60000]\n",
            "loss: 0.028933 [51000/60000]\n",
            "loss: 0.056465 [52000/60000]\n",
            "loss: 0.155930 [53000/60000]\n",
            "loss: 5.558147 [54000/60000]\n",
            "loss: 0.014066 [55000/60000]\n",
            "loss: 0.022052 [56000/60000]\n",
            "loss: 1.971496 [57000/60000]\n",
            "loss: 0.001652 [58000/60000]\n",
            "loss: 0.267171 [59000/60000]\n",
            "loss: 0.155886 [60000/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.9%, Avg loss: 0.330345 \n",
            "\n",
            "Epoch 9\n",
            "-----------------------------------\n",
            "loss: 0.082092 [ 1000/60000]\n",
            "loss: 0.014784 [ 2000/60000]\n",
            "loss: 0.018817 [ 3000/60000]\n",
            "loss: 0.013774 [ 4000/60000]\n",
            "loss: 0.037760 [ 5000/60000]\n",
            "loss: 0.103580 [ 6000/60000]\n",
            "loss: 0.126272 [ 7000/60000]\n",
            "loss: 0.012972 [ 8000/60000]\n",
            "loss: 0.131158 [ 9000/60000]\n",
            "loss: 0.046619 [10000/60000]\n",
            "loss: 0.002470 [11000/60000]\n",
            "loss: 1.071843 [12000/60000]\n",
            "loss: 0.511387 [13000/60000]\n",
            "loss: 0.113991 [14000/60000]\n",
            "loss: 0.807858 [15000/60000]\n",
            "loss: 0.063438 [16000/60000]\n",
            "loss: 0.057371 [17000/60000]\n",
            "loss: 0.071908 [18000/60000]\n",
            "loss: 0.351897 [19000/60000]\n",
            "loss: 0.028455 [20000/60000]\n",
            "loss: 0.621127 [21000/60000]\n",
            "loss: 0.089284 [22000/60000]\n",
            "loss: 0.046439 [23000/60000]\n",
            "loss: 0.059307 [24000/60000]\n",
            "loss: 0.405297 [25000/60000]\n",
            "loss: 0.019232 [26000/60000]\n",
            "loss: 0.024320 [27000/60000]\n",
            "loss: 0.007663 [28000/60000]\n",
            "loss: 0.038307 [29000/60000]\n",
            "loss: 0.148052 [30000/60000]\n",
            "loss: 0.133844 [31000/60000]\n",
            "loss: 0.096744 [32000/60000]\n",
            "loss: 0.037427 [33000/60000]\n",
            "loss: 0.543111 [34000/60000]\n",
            "loss: 0.023671 [35000/60000]\n",
            "loss: 0.141486 [36000/60000]\n",
            "loss: 0.454063 [37000/60000]\n",
            "loss: 0.033474 [38000/60000]\n",
            "loss: 0.177596 [39000/60000]\n",
            "loss: 2.775000 [40000/60000]\n",
            "loss: 0.020159 [41000/60000]\n",
            "loss: 0.086658 [42000/60000]\n",
            "loss: 0.015512 [43000/60000]\n",
            "loss: 0.728011 [44000/60000]\n",
            "loss: 0.027341 [45000/60000]\n",
            "loss: 0.039539 [46000/60000]\n",
            "loss: 0.810473 [47000/60000]\n",
            "loss: 0.166706 [48000/60000]\n",
            "loss: 0.095152 [49000/60000]\n",
            "loss: 0.125359 [50000/60000]\n",
            "loss: 0.022609 [51000/60000]\n",
            "loss: 0.072098 [52000/60000]\n",
            "loss: 0.105182 [53000/60000]\n",
            "loss: 5.855340 [54000/60000]\n",
            "loss: 0.012386 [55000/60000]\n",
            "loss: 0.015119 [56000/60000]\n",
            "loss: 1.711912 [57000/60000]\n",
            "loss: 0.001193 [58000/60000]\n",
            "loss: 0.181929 [59000/60000]\n",
            "loss: 0.127673 [60000/60000]\n",
            "Test Error: \n",
            " Accuracy: 90.9%, Avg loss: 0.295626 \n",
            "\n",
            "Epoch 10\n",
            "-----------------------------------\n",
            "loss: 0.060188 [ 1000/60000]\n",
            "loss: 0.009188 [ 2000/60000]\n",
            "loss: 0.009762 [ 3000/60000]\n",
            "loss: 0.012298 [ 4000/60000]\n",
            "loss: 0.029100 [ 5000/60000]\n",
            "loss: 0.081686 [ 6000/60000]\n",
            "loss: 0.103712 [ 7000/60000]\n",
            "loss: 0.008977 [ 8000/60000]\n",
            "loss: 0.103744 [ 9000/60000]\n",
            "loss: 0.040098 [10000/60000]\n",
            "loss: 0.001635 [11000/60000]\n",
            "loss: 1.165776 [12000/60000]\n",
            "loss: 0.450922 [13000/60000]\n",
            "loss: 0.117441 [14000/60000]\n",
            "loss: 0.905289 [15000/60000]\n",
            "loss: 0.056832 [16000/60000]\n",
            "loss: 0.043310 [17000/60000]\n",
            "loss: 0.057438 [18000/60000]\n",
            "loss: 0.307739 [19000/60000]\n",
            "loss: 0.023085 [20000/60000]\n",
            "loss: 0.604264 [21000/60000]\n",
            "loss: 0.054096 [22000/60000]\n",
            "loss: 0.031649 [23000/60000]\n",
            "loss: 0.033250 [24000/60000]\n",
            "loss: 0.331178 [25000/60000]\n",
            "loss: 0.016672 [26000/60000]\n",
            "loss: 0.016936 [27000/60000]\n",
            "loss: 0.005067 [28000/60000]\n",
            "loss: 0.023554 [29000/60000]\n",
            "loss: 0.130145 [30000/60000]\n",
            "loss: 0.100507 [31000/60000]\n",
            "loss: 0.057279 [32000/60000]\n",
            "loss: 0.028072 [33000/60000]\n",
            "loss: 0.439095 [34000/60000]\n",
            "loss: 0.017121 [35000/60000]\n",
            "loss: 0.109166 [36000/60000]\n",
            "loss: 0.434711 [37000/60000]\n",
            "loss: 0.031354 [38000/60000]\n",
            "loss: 0.131846 [39000/60000]\n",
            "loss: 2.822265 [40000/60000]\n",
            "loss: 0.015820 [41000/60000]\n",
            "loss: 0.087274 [42000/60000]\n",
            "loss: 0.013603 [43000/60000]\n",
            "loss: 0.692428 [44000/60000]\n",
            "loss: 0.021817 [45000/60000]\n",
            "loss: 0.032502 [46000/60000]\n",
            "loss: 0.556112 [47000/60000]\n",
            "loss: 0.152408 [48000/60000]\n",
            "loss: 0.094821 [49000/60000]\n",
            "loss: 0.103710 [50000/60000]\n",
            "loss: 0.018307 [51000/60000]\n",
            "loss: 0.085352 [52000/60000]\n",
            "loss: 0.082543 [53000/60000]\n",
            "loss: 6.047339 [54000/60000]\n",
            "loss: 0.011418 [55000/60000]\n",
            "loss: 0.011653 [56000/60000]\n",
            "loss: 1.439834 [57000/60000]\n",
            "loss: 0.000934 [58000/60000]\n",
            "loss: 0.124728 [59000/60000]\n",
            "loss: 0.114515 [60000/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.5%, Avg loss: 0.270262 \n",
            "\n",
            "Done!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U5SbDwS6IZQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}