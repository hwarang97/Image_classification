{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4pI+bFUA0kgy6fqa+GB+c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwarang97/Image_classification/blob/main/VGG16_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG\n",
        "- 16, 19 등 뒤에 붙는 숫자는 구성된 층의 갯수\n",
        "- 쉬운 구조와 성능 덕분에 우승모델(Googlenet)보다 인기가 더 많음\n",
        "- 핵심\n",
        "  - 모델 깊이와 성능의 관계 파악\n",
        "    - 더 깊이 만들기 위해서 커널을 3x3을 고정\n",
        "    - 큰 필터를 한번 적용하는것보다 작은 필터를 여러번 적용하는것이 성능은 같은데 파라미터 수가 더 적음\n",
        "    - 작은 필터를 적용하니 비선형성이 증가하여 특성을 잘 나타냄\n",
        "\n",
        "- input : 224 x 224 x 3"
      ],
      "metadata": {
        "id": "g73J0uCGneGK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iEDL_XQ3XrNE"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.activation import Softmax \n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import * \n",
        "from torch.nn.modules import dropout\n",
        "from torch.nn.modules.adaptive import Linear\n",
        "from torch.nn.modules.pooling import MaxPool2d\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' "
      ],
      "metadata": {
        "id": "oLB1Wot7rbPJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve data directly from Stanford data source\n",
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "\n",
        "# Unzip raw zip file\n",
        "!unzip -qq 'tiny-imagenet-200.zip'"
      ],
      "metadata": {
        "id": "KsSiZSibwiPP",
        "outputId": "be9576c3-123a-43f1-8088-e3d6ef570ea7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-29 13:33:40--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  38.8MB/s    in 7.3s    \n",
            "\n",
            "2022-12-29 13:33:48 (32.2 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define main data directory\n",
        "DATA_DIR = 'tiny-imagenet-200'\n",
        "\n",
        "# Define training and validation data paths\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
        "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
        "\n",
        "training_data = datasets.ImageFolder(root = TRAIN_DIR, \n",
        "                                     transform=transforms.Compose([ # transform : 이미지 특징들에 적용\n",
        "                                         Resize(224),\n",
        "                                         ToTensor(), # 텐서로 만들고, 0~1 사이 값으로 스케일링\n",
        "                                     ]))\n",
        "test_data = datasets.ImageFolder(root = TEST_DIR, transform=transforms.Compose([\n",
        "                                         Resize(224),\n",
        "                                         ToTensor(),\n",
        "                                     ]))\n",
        "\n",
        "sample = training_data[0][0]\n",
        "print(sample.shape, sample.dtype, sample.max(), sample.min()) # 제대로 변환되었는지 확인"
      ],
      "metadata": {
        "id": "8u4N9MgW4eAk",
        "outputId": "4719b6b0-5031-49a5-c013-47b605003ba1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224]) torch.float32 tensor(1.) tensor(0.0235)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(training_data, batch_size=32) # batch_size가 작으니까 너무 불안정하다. 128정도 이상은 되야하지 않을까\n",
        "test_dataloader = DataLoader(test_data, batch_size=32)"
      ],
      "metadata": {
        "id": "FNfWP2qbd4lv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 추가학습\n",
        "- 직접 데이터셋을 구성하고 불러오는 방법\n",
        "- https://data-panic.tistory.com/13\n",
        "\n",
        "- 참조한 페이지(tiny-imagenet-200 관련)\n",
        "- https://towardsdatascience.com/pytorch-ignite-classifying-tiny-imagenet-with-efficientnet-e5b1768e5e8f"
      ],
      "metadata": {
        "id": "Q209PSg74rV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    \"\"\"Conv layers\"\"\"\n",
        "    self.conv_relu_stack = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, 3, padding=1),   # conv1_1\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, 3, padding=1),  # conv1_2\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(64, 128, 3, padding=1), # conv2_1\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128, 128, 3, padding=1), # conv2_2\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(128, 256, 3, padding=1), # conv3_1\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(256, 256, 3, padding=1), # conv3_2\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(256, 256, 3, padding=1), # conv3_3\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(256, 512, 3, padding=1), # conv4_1\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512, 512, 3, padding=1), # conv4_2\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512, 512, 3, padding=1), # conv4_3\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),    \n",
        "        nn.Conv2d(512, 512, 3, padding=1), # conv5_1\n",
        "        nn.ReLU(),           \n",
        "        nn.Conv2d(512, 512, 3, padding=1), # conv5_2\n",
        "        nn.ReLU(),          \n",
        "        nn.Conv2d(512, 512, 3, padding=1), # conv5_3\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "    )\n",
        "\n",
        "    \"\"\"FC layers\"\"\"\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(7*7*512, 4096),  \n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),        # 드롭 아웃은 왜 활성화 다음에 쓸까, 전에 쓰면 계산할것도 줄어들지 않나\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(4096,1000)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.conv_relu_stack(x)\n",
        "    x2 = self.flatten(x1)\n",
        "    logits = self.linear_relu_stack(x2)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "B6B6Wx-Mr0Kf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for step, (X, y) in enumerate(dataloader, 1): # 전체 이미지를 batch_size만큼 묶어서 전달\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    optimizer.zero_grad() # 전에 역전파로 얻었던 w 기울기를 0으로 만들어주기. 안그러면 누적되어 점점커짐\n",
        "    loss.backward() # 역전파\n",
        "    optimizer.step() # 오차에 기여한 만큼 파라미터를 갱신\n",
        "\n",
        "    if step % 10 == 0:\n",
        "      loss, current = loss.item(), step * len(X)\n",
        "      print(f'loss : {loss:>7f} [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad(): # grad를 저장하지 않음\n",
        "    for X, y in dataloader:\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item() # 오차값을 누적\n",
        "      correct += (pred.argmax(1)==y).type(torch.float).sum().item() # 몇개 맞췄는지 기록\n",
        "\n",
        "  test_loss /= num_batches # loss 값의 평균\n",
        "  correct /= size          # 정확도 평균\n",
        "  print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n') # 0.1f는 뭐지 -> 소수점 한자리까지 표현(?)"
      ],
      "metadata": {
        "id": "HqcrN4y2X9YZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG16().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "U2d-A4hAbl5z",
        "outputId": "9bba534e-a449-4da7-902e-08e937ff245d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG16(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (conv_relu_stack): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU()\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU()\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU()\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU()\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU()\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU()\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU()\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU()\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU()\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU()\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU()\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 추가 학습\n",
        "- print f format의 정렬기능\n",
        "  - 중괄호{}안에 변수 옆에 : 를 입력\n",
        "  - < : 왼쪽정렬\n",
        "  - \\> : 오른정렬\n",
        "  - \\^ : 중앙정렬\n",
        "  - 숫자 : 몇칸으로 표현할지"
      ],
      "metadata": {
        "id": "mYlU6HlUY-1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-2\n",
        "epochs = 1\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # 미니배치면 불완전하지만 SDG보다는 빠르게 최적 파라미에 도달한다\n",
        "\n",
        "for t in range(epochs):\n",
        "  print(f'Epoch {t+1}\\n-----------------------------')\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "print('DONE!!!!!!!!!!!!!!!!!!!!!!!') # GPU RAM 값이 부족하면 kernel을 restart해보고, 그래도 찼다면 batch_size를 줄이고 이걸 반복"
      ],
      "metadata": {
        "id": "KrSrapIocv-_",
        "outputId": "ba9f1064-d683-450e-f6dc-4cf016f8a4a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-----------------------------\n",
            "loss : 6.775857 [  320/100000]\n",
            "loss : 6.854997 [  640/100000]\n",
            "loss : 6.713550 [  960/100000]\n",
            "loss : 6.812537 [ 1280/100000]\n",
            "loss : 6.888543 [ 1600/100000]\n",
            "loss : 6.743188 [ 1920/100000]\n",
            "loss : 6.814485 [ 2240/100000]\n",
            "loss : 6.891642 [ 2560/100000]\n",
            "loss : 6.743404 [ 2880/100000]\n",
            "loss : 6.846145 [ 3200/100000]\n",
            "loss : 6.819551 [ 3520/100000]\n",
            "loss : 6.748758 [ 3840/100000]\n",
            "loss : 6.846713 [ 4160/100000]\n",
            "loss : 6.690723 [ 4480/100000]\n",
            "loss : 6.764525 [ 4800/100000]\n",
            "loss : 6.847092 [ 5120/100000]\n",
            "loss : 6.681594 [ 5440/100000]\n",
            "loss : 6.782244 [ 5760/100000]\n",
            "loss : 6.902182 [ 6080/100000]\n",
            "loss : 6.731118 [ 6400/100000]\n",
            "loss : 6.814731 [ 6720/100000]\n",
            "loss : 6.912586 [ 7040/100000]\n",
            "loss : 6.725810 [ 7360/100000]\n",
            "loss : 6.822799 [ 7680/100000]\n",
            "loss : 6.621541 [ 8000/100000]\n",
            "loss : 6.705748 [ 8320/100000]\n",
            "loss : 6.850522 [ 8640/100000]\n",
            "loss : 6.616325 [ 8960/100000]\n",
            "loss : 6.721533 [ 9280/100000]\n",
            "loss : 6.850364 [ 9600/100000]\n",
            "loss : 6.530475 [ 9920/100000]\n",
            "loss : 6.657182 [10240/100000]\n",
            "loss : 6.852275 [10560/100000]\n",
            "loss : 6.062894 [10880/100000]\n",
            "loss : 5.845178 [11200/100000]\n",
            "loss : 7.584822 [11520/100000]\n",
            "loss : 0.026008 [11840/100000]\n",
            "loss : 1.811157 [12160/100000]\n",
            "loss : 0.013619 [12480/100000]\n",
            "loss : 0.038576 [12800/100000]\n",
            "loss : 3.053650 [13120/100000]\n",
            "loss : 0.015296 [13440/100000]\n",
            "loss : 0.044186 [13760/100000]\n",
            "loss : 4.865232 [14080/100000]\n",
            "loss : 0.022135 [14400/100000]\n",
            "loss : 0.129365 [14720/100000]\n",
            "loss : 9.683543 [15040/100000]\n",
            "loss : 0.027983 [15360/100000]\n",
            "loss : 0.510468 [15680/100000]\n",
            "loss : 0.016272 [16000/100000]\n",
            "loss : 0.046909 [16320/100000]\n",
            "loss : 3.068436 [16640/100000]\n",
            "loss : 0.018394 [16960/100000]\n",
            "loss : 0.060242 [17280/100000]\n",
            "loss : 4.482529 [17600/100000]\n",
            "loss : 0.023879 [17920/100000]\n",
            "loss : 0.115380 [18240/100000]\n",
            "loss : 5.974625 [18560/100000]\n",
            "loss : 0.032950 [18880/100000]\n",
            "loss : 0.661956 [19200/100000]\n",
            "loss : 8.081822 [19520/100000]\n",
            "loss : 0.052122 [19840/100000]\n",
            "loss : 2.270979 [20160/100000]\n",
            "loss : 0.022701 [20480/100000]\n",
            "loss : 0.070406 [20800/100000]\n",
            "loss : 3.889601 [21120/100000]\n",
            "loss : 0.027600 [21440/100000]\n",
            "loss : 0.136984 [21760/100000]\n",
            "loss : 5.365522 [22080/100000]\n",
            "loss : 0.036843 [22400/100000]\n",
            "loss : 0.346646 [22720/100000]\n",
            "loss : 9.891508 [23040/100000]\n",
            "loss : 0.046658 [23360/100000]\n",
            "loss : 1.820910 [23680/100000]\n",
            "loss : 0.025568 [24000/100000]\n",
            "loss : 0.070457 [24320/100000]\n",
            "loss : 3.629293 [24640/100000]\n",
            "loss : 0.030770 [24960/100000]\n",
            "loss : 0.131795 [25280/100000]\n",
            "loss : 5.011574 [25600/100000]\n",
            "loss : 0.038041 [25920/100000]\n",
            "loss : 0.383417 [26240/100000]\n",
            "loss : 6.417475 [26560/100000]\n",
            "loss : 0.045473 [26880/100000]\n",
            "loss : 1.408873 [27200/100000]\n",
            "loss : 7.910246 [27520/100000]\n",
            "loss : 0.062237 [27840/100000]\n",
            "loss : 2.842013 [28160/100000]\n",
            "loss : 0.032355 [28480/100000]\n",
            "loss : 0.126525 [28800/100000]\n",
            "loss : 4.521658 [29120/100000]\n",
            "loss : 0.041764 [29440/100000]\n",
            "loss : 0.234269 [29760/100000]\n",
            "loss : 5.790692 [30080/100000]\n",
            "loss : 0.047740 [30400/100000]\n",
            "loss : 0.805162 [30720/100000]\n",
            "loss : 10.164403 [31040/100000]\n",
            "loss : 0.067562 [31360/100000]\n",
            "loss : 2.559981 [31680/100000]\n",
            "loss : 0.030288 [32000/100000]\n",
            "loss : 0.094393 [32320/100000]\n",
            "loss : 4.135007 [32640/100000]\n",
            "loss : 0.035867 [32960/100000]\n",
            "loss : 0.264588 [33280/100000]\n",
            "loss : 5.241772 [33600/100000]\n",
            "loss : 0.049676 [33920/100000]\n",
            "loss : 0.804640 [34240/100000]\n",
            "loss : 6.831491 [34560/100000]\n",
            "loss : 0.064773 [34880/100000]\n",
            "loss : 1.930325 [35200/100000]\n",
            "loss : 7.773225 [35520/100000]\n",
            "loss : 0.099753 [35840/100000]\n",
            "loss : 3.642949 [36160/100000]\n",
            "loss : 0.039568 [36480/100000]\n",
            "loss : 0.193230 [36800/100000]\n",
            "loss : 4.703491 [37120/100000]\n",
            "loss : 0.047941 [37440/100000]\n",
            "loss : 0.457427 [37760/100000]\n",
            "loss : 5.751773 [38080/100000]\n",
            "loss : 0.054757 [38400/100000]\n",
            "loss : 1.330285 [38720/100000]\n",
            "loss : 10.503049 [39040/100000]\n",
            "loss : 0.086217 [39360/100000]\n",
            "loss : 2.918339 [39680/100000]\n",
            "loss : 0.029566 [40000/100000]\n",
            "loss : 0.152865 [40320/100000]\n",
            "loss : 4.329189 [40640/100000]\n",
            "loss : 0.052082 [40960/100000]\n",
            "loss : 0.274657 [41280/100000]\n",
            "loss : 5.429776 [41600/100000]\n",
            "loss : 0.044899 [41920/100000]\n",
            "loss : 1.198099 [42240/100000]\n",
            "loss : 6.970613 [42560/100000]\n",
            "loss : 0.082464 [42880/100000]\n",
            "loss : 2.285619 [43200/100000]\n",
            "loss : 7.854069 [43520/100000]\n",
            "loss : 0.129951 [43840/100000]\n",
            "loss : 3.723640 [44160/100000]\n",
            "loss : 0.051633 [44480/100000]\n",
            "loss : 0.266101 [44800/100000]\n",
            "loss : 4.786189 [45120/100000]\n",
            "loss : 0.052664 [45440/100000]\n",
            "loss : 0.813847 [45760/100000]\n",
            "loss : 6.093226 [46080/100000]\n",
            "loss : 0.067255 [46400/100000]\n",
            "loss : 1.862810 [46720/100000]\n",
            "loss : 10.409126 [47040/100000]\n",
            "loss : 0.104009 [47360/100000]\n",
            "loss : 3.205542 [47680/100000]\n",
            "loss : 0.041209 [48000/100000]\n",
            "loss : 0.197713 [48320/100000]\n",
            "loss : 4.343132 [48640/100000]\n",
            "loss : 0.055618 [48960/100000]\n",
            "loss : 0.485038 [49280/100000]\n",
            "loss : 5.334996 [49600/100000]\n",
            "loss : 0.073177 [49920/100000]\n",
            "loss : 1.420428 [50240/100000]\n",
            "loss : 7.354550 [50560/100000]\n",
            "loss : 0.111875 [50880/100000]\n",
            "loss : 2.710273 [51200/100000]\n",
            "loss : 7.653856 [51520/100000]\n",
            "loss : 0.165691 [51840/100000]\n",
            "loss : 3.798667 [52160/100000]\n",
            "loss : 0.044381 [52480/100000]\n",
            "loss : 0.334334 [52800/100000]\n",
            "loss : 4.934445 [53120/100000]\n",
            "loss : 0.060900 [53440/100000]\n",
            "loss : 0.894937 [53760/100000]\n",
            "loss : 6.293868 [54080/100000]\n",
            "loss : 0.099148 [54400/100000]\n",
            "loss : 2.079176 [54720/100000]\n",
            "loss : 10.224572 [55040/100000]\n",
            "loss : 0.135353 [55360/100000]\n",
            "loss : 3.119355 [55680/100000]\n",
            "loss : 0.047239 [56000/100000]\n",
            "loss : 0.247516 [56320/100000]\n",
            "loss : 4.489421 [56640/100000]\n",
            "loss : 0.069565 [56960/100000]\n",
            "loss : 0.561169 [57280/100000]\n",
            "loss : 5.592068 [57600/100000]\n",
            "loss : 0.093375 [57920/100000]\n",
            "loss : 1.705495 [58240/100000]\n",
            "loss : 7.543026 [58560/100000]\n",
            "loss : 0.135634 [58880/100000]\n",
            "loss : 2.973680 [59200/100000]\n",
            "loss : 7.637081 [59520/100000]\n",
            "loss : 0.227030 [59840/100000]\n",
            "loss : 3.930635 [60160/100000]\n",
            "loss : 0.065420 [60480/100000]\n",
            "loss : 0.459198 [60800/100000]\n",
            "loss : 5.249163 [61120/100000]\n",
            "loss : 0.093820 [61440/100000]\n",
            "loss : 1.391154 [61760/100000]\n",
            "loss : 6.548450 [62080/100000]\n",
            "loss : 0.116289 [62400/100000]\n",
            "loss : 2.221416 [62720/100000]\n",
            "loss : 10.562673 [63040/100000]\n",
            "loss : 0.199475 [63360/100000]\n",
            "loss : 3.360625 [63680/100000]\n",
            "loss : 0.058853 [64000/100000]\n",
            "loss : 0.314692 [64320/100000]\n",
            "loss : 4.593901 [64640/100000]\n",
            "loss : 0.088274 [64960/100000]\n",
            "loss : 0.856306 [65280/100000]\n",
            "loss : 5.718910 [65600/100000]\n",
            "loss : 0.102420 [65920/100000]\n",
            "loss : 1.783691 [66240/100000]\n",
            "loss : 7.663482 [66560/100000]\n",
            "loss : 0.137058 [66880/100000]\n",
            "loss : 2.999898 [67200/100000]\n",
            "loss : 7.255426 [67520/100000]\n",
            "loss : 0.231150 [67840/100000]\n",
            "loss : 4.219789 [68160/100000]\n",
            "loss : 0.069730 [68480/100000]\n",
            "loss : 0.552150 [68800/100000]\n",
            "loss : 5.114986 [69120/100000]\n",
            "loss : 0.095120 [69440/100000]\n",
            "loss : 1.183276 [69760/100000]\n",
            "loss : 6.496601 [70080/100000]\n",
            "loss : 0.115489 [70400/100000]\n",
            "loss : 2.379507 [70720/100000]\n",
            "loss : 10.665875 [71040/100000]\n",
            "loss : 0.203493 [71360/100000]\n",
            "loss : 3.607228 [71680/100000]\n",
            "loss : 0.061873 [72000/100000]\n",
            "loss : 0.399009 [72320/100000]\n",
            "loss : 4.682980 [72640/100000]\n",
            "loss : 0.087239 [72960/100000]\n",
            "loss : 0.780973 [73280/100000]\n",
            "loss : 5.803829 [73600/100000]\n",
            "loss : 0.113268 [73920/100000]\n",
            "loss : 1.789907 [74240/100000]\n",
            "loss : 7.708229 [74560/100000]\n",
            "loss : 0.161307 [74880/100000]\n",
            "loss : 3.114424 [75200/100000]\n",
            "loss : 7.540359 [75520/100000]\n",
            "loss : 0.297057 [75840/100000]\n",
            "loss : 4.029093 [76160/100000]\n",
            "loss : 0.078231 [76480/100000]\n",
            "loss : 0.639087 [76800/100000]\n",
            "loss : 5.293344 [77120/100000]\n",
            "loss : 0.090079 [77440/100000]\n",
            "loss : 1.313510 [77760/100000]\n",
            "loss : 6.591704 [78080/100000]\n",
            "loss : 0.117483 [78400/100000]\n",
            "loss : 2.461377 [78720/100000]\n",
            "loss : 10.400146 [79040/100000]\n",
            "loss : 0.181129 [79360/100000]\n",
            "loss : 3.581357 [79680/100000]\n",
            "loss : 0.067527 [80000/100000]\n",
            "loss : 0.430660 [80320/100000]\n",
            "loss : 4.522260 [80640/100000]\n",
            "loss : 0.085953 [80960/100000]\n",
            "loss : 0.741103 [81280/100000]\n",
            "loss : 5.901624 [81600/100000]\n",
            "loss : 0.095671 [81920/100000]\n",
            "loss : 1.898803 [82240/100000]\n",
            "loss : 7.762215 [82560/100000]\n",
            "loss : 0.153061 [82880/100000]\n",
            "loss : 2.933790 [83200/100000]\n",
            "loss : 7.137083 [83520/100000]\n",
            "loss : 0.266442 [83840/100000]\n",
            "loss : 4.063780 [84160/100000]\n",
            "loss : 0.077704 [84480/100000]\n",
            "loss : 0.687108 [84800/100000]\n",
            "loss : 5.304688 [85120/100000]\n",
            "loss : 0.100805 [85440/100000]\n",
            "loss : 1.364163 [85760/100000]\n",
            "loss : 6.647402 [86080/100000]\n",
            "loss : 0.139232 [86400/100000]\n",
            "loss : 2.614492 [86720/100000]\n",
            "loss : 10.670366 [87040/100000]\n",
            "loss : 0.212950 [87360/100000]\n",
            "loss : 3.715518 [87680/100000]\n",
            "loss : 0.062841 [88000/100000]\n",
            "loss : 0.412776 [88320/100000]\n",
            "loss : 4.809787 [88640/100000]\n",
            "loss : 0.102962 [88960/100000]\n",
            "loss : 0.814528 [89280/100000]\n",
            "loss : 5.842930 [89600/100000]\n",
            "loss : 0.116078 [89920/100000]\n",
            "loss : 2.001409 [90240/100000]\n",
            "loss : 7.867754 [90560/100000]\n",
            "loss : 0.212477 [90880/100000]\n",
            "loss : 3.288720 [91200/100000]\n",
            "loss : 7.133353 [91520/100000]\n",
            "loss : 0.282634 [91840/100000]\n",
            "loss : 4.081004 [92160/100000]\n",
            "loss : 0.100343 [92480/100000]\n",
            "loss : 0.667445 [92800/100000]\n",
            "loss : 5.377820 [93120/100000]\n",
            "loss : 0.104598 [93440/100000]\n",
            "loss : 1.481271 [93760/100000]\n",
            "loss : 6.573602 [94080/100000]\n",
            "loss : 0.130225 [94400/100000]\n",
            "loss : 2.405231 [94720/100000]\n",
            "loss : 10.424770 [95040/100000]\n",
            "loss : 0.206246 [95360/100000]\n",
            "loss : 3.817716 [95680/100000]\n",
            "loss : 0.084348 [96000/100000]\n",
            "loss : 0.417398 [96320/100000]\n",
            "loss : 4.616201 [96640/100000]\n",
            "loss : 0.098880 [96960/100000]\n",
            "loss : 0.918264 [97280/100000]\n",
            "loss : 5.658138 [97600/100000]\n",
            "loss : 0.111300 [97920/100000]\n",
            "loss : 1.823810 [98240/100000]\n",
            "loss : 7.800299 [98560/100000]\n",
            "loss : 0.172929 [98880/100000]\n",
            "loss : 3.063563 [99200/100000]\n",
            "loss : 7.298786 [99520/100000]\n",
            "loss : 0.230120 [99840/100000]\n",
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 11.673677 \n",
            "\n",
            "DONE!!!!!!!!!!!!!!!!!!!!!!!\n"
          ]
        }
      ]
    }
  ]
}